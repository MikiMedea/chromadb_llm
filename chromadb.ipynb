{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1428159,
          "sourceType": "datasetVersion",
          "datasetId": 836401
        },
        {
          "sourceId": 6104553,
          "sourceType": "datasetVersion",
          "datasetId": 3496946
        },
        {
          "sourceId": 10093095,
          "sourceType": "datasetVersion",
          "datasetId": 1977878
        }
      ],
      "dockerImageVersionId": 30527,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook41d21fac63",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikiMedea/chromadb_llm/blob/main/chromadb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "kotartemiy_topic_labeled_news_dataset_path = kagglehub.dataset_download('kotartemiy/topic-labeled-news-dataset')\n",
        "deepanshudalal09_mit_ai_news_published_till_2023_path = kagglehub.dataset_download('deepanshudalal09/mit-ai-news-published-till-2023')\n",
        "gpreda_bbc_news_path = kagglehub.dataset_download('gpreda/bbc-news')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Uo9dVaGW3HU6"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div align=\"center\">\n",
        "<h1><a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course\">Learn by Doing LLM Projects</a></h1>\n",
        "    <h3>Understand And Apply Large Language Models</h3>\n",
        "    <h2>HOW TO USE A VECTOR / EMBEDDING DATABASE TO PROVIDE CONTEXT TO A LARGE LANGUAGE MODEL</h2>\n",
        "    by <b>Pere Martra</b>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    &nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/pere-martra/\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T22:11:51.956945Z",
          "iopub.execute_input": "2023-10-25T22:11:51.957387Z",
          "iopub.status.idle": "2023-10-25T22:11:51.965701Z",
          "shell.execute_reply.started": "2023-10-25T22:11:51.957351Z",
          "shell.execute_reply": "2023-10-25T22:11:51.964581Z"
        },
        "id": "Ent-J5p13HU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This notebook is part of a comprehensive course on Large Language Models available on GitHub: https://github.com/peremartra/Large-Language-Model-Notebooks-Course. If you want to stay informed about new lessons or updates, simply follow or star the repository.\n",
        "\n",
        "In this notebook we will see how to use an embedding database to store the information that we want to pass to a large language model so that it takes it into account in its responses.\n",
        "\n",
        "The information could be our own documents, or whatever was contained in a business knowledge database.\n",
        "\n",
        "I have prepared the notebook so that it can work with three different Kaggle datasets, so that it is easy to carry out different tests with different Datasets.\n",
        "\n",
        "![VectorDatabase.png](attachment:a73106c8-f286-41fc-b9d7-66638412114c.png)\n",
        "### Feel Free to fork or edit the noteboook for you own convenience. Please consider ***UPVOTING IT***. It helps others to discover the notebook, and it encourages me to continue publishing."
      ],
      "metadata": {
        "id": "Z48BLvmm3HU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and load the libraries.\n",
        "To start we need to install the necesary Python packages.\n",
        "* **[sentence transformers](http:/www.sbert.net/)**. This library is necessary to transform the sentences into fixed-length vectors, also know as embeddings.\n",
        "* **[xformers](https://github.com/facebookresearch/xformers)**. it's a package that provides libraries an utilities to facilitate the work with transformers models. We need to install in order to avoid an error when we work with the model and embeddings.  \n",
        "* **[chromadb](https://www.trychroma.com/)**. This is our vector Database. ChromaDB is easy to use and open source, maybe the most used Vector Database used to store embeddings."
      ],
      "metadata": {
        "id": "k7-mBw2g3HU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers\n",
        "# !pip install -q sentence-transformers\n",
        "#!pip install -q xformers\n",
        "# !pip install -q chromadb"
      ],
      "metadata": {
        "trusted": true,
        "id": "_0JupOA83HU8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.41.2\n",
        "!pip install chromadb==0.4.20"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T16:57:27.517663Z",
          "iopub.execute_input": "2025-01-21T16:57:27.518883Z",
          "execution_failed": "2025-01-21T16:59:22.361Z"
        },
        "id": "gOkP-NQ_3HU8",
        "outputId": "5737c8b2-00b3-4ff3-b203-2980cbbcc923"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a3ef97185e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a3ef9718910>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a3ef9718bb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm sure that you know the next two packages: Numpy and Pandas, maybe the most used python libraries.\n",
        "\n",
        "Numpy is a powerful library for numerical computing.\n",
        "\n",
        "Pandas is a library for data manipulation"
      ],
      "metadata": {
        "id": "XXoIinNl3HU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "trusted": true,
        "id": "9bkrK4Ge3HU9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset\n",
        "As you can see the notebook is ready to work with three different Datasets. Just uncomment the lines of the Dataset you want to use.\n",
        "\n",
        "I selected Datasets with News. Two of them have just a brief decription of the new, but the other contains the full text.\n",
        "\n",
        "As we are working in a free and limited space, and we can use just 30 gb of memory I limited the number of news to use with the variable MAX_NEWS.\n",
        "\n",
        "The name of the field containing the text of the new is stored in the variable *DOCUMENT* and the metadata in *TOPIC*"
      ],
      "metadata": {
        "id": "NTpluhyd3HU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = pd.read_csv('/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv', sep=';')\n",
        "MAX_NEWS = 1000\n",
        "DOCUMENT=\"title\"\n",
        "TOPIC=\"topic\"\n",
        "\n",
        "#news = pd.read_csv('/kaggle/input/bbc-news/bbc_news.csv')\n",
        "#MAX_NEWS = 1000\n",
        "#DOCUMENT=\"description\"\n",
        "#TOPIC=\"title\"\n",
        "\n",
        "#news = pd.read_csv('/kaggle/input/mit-ai-news-published-till-2023/articles.csv')\n",
        "#MAX_NEWS = 100\n",
        "#DOCUMENT=\"Article Body\"\n",
        "#TOPIC=\"Article Header\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5mkLIs2j3HU9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChromaDB requires that the data has a unique identifier. We can make it with this statement, which will create a new column called **Id**.\n"
      ],
      "metadata": {
        "id": "jsfNTQE63HU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news[\"id\"] = news.index\n",
        "news.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "1J8y7E0g3HU9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Because it is just a course we select a small portion of News.\n",
        "subset_news = news.head(MAX_NEWS)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DIF6wXoe3HU9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and configure the Vector Database\n",
        "I'm going to use ChromaDB, the most popular OpenSource embedding Database.\n",
        "\n",
        "First we need to import ChromaDB, and after that import the **Settings** class from **chromadb.config** module. This class allows us to change the setting for the ChromaDB system, and customize its behavior."
      ],
      "metadata": {
        "id": "2w1yCqvH3HU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings"
      ],
      "metadata": {
        "trusted": true,
        "id": "10ia5heB3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create the seetings object calling the **Settings** function imported previously. We store the object in the variable **settings_chroma**.\n",
        "\n",
        "Is necessary to inform two parameters\n",
        "* chroma_db_impl. Here we specify the database implementation and the format how store the data. I choose ***duckdb***, because his high-performace. It operate primarly in memory. And is fully compatible with SQL. The store format ***parquet*** is good for tabular data. With good compression rates and performance.\n",
        "\n",
        "* persist_directory: It just contains the directory where the data will be stored. Is possible work without a directory and the data will be stored in memory without persistece, but Kaggle dosn't support that."
      ],
      "metadata": {
        "id": "hExHPceH3HU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OLD VERSION\n",
        "#settings_chroma = Settings(chroma_db_impl=\"duckdb+parquet\",\n",
        "#                          persist_directory='./input')\n",
        "#chroma_client = chromadb.Client(settings_chroma)\n",
        "\n",
        "#NEW VERSION => 0.40\n",
        "chroma_client = chromadb.PersistentClient(path=\"/kaggle/working\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "FKLL1fqi3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filling and Querying the ChromaDB Database\n",
        "The Data in ChromaDB is stored in collections. If the collection exist we need to delete it.\n",
        "\n",
        "In the next lines, we are creating the collection by calling the ***create_collection*** function in the ***chroma_client*** created above."
      ],
      "metadata": {
        "id": "zSQfUISk3HU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "trusted": true,
        "id": "HOHmU8Bc3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"news_collection\"+datetime.now().strftime(\"%s\")\n",
        "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
        "        chroma_client.delete_collection(name=collection_name)\n",
        "\n",
        "collection = chroma_client.create_collection(name=collection_name)\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "Lw63-uMm3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to add the data to the collection. Using the function ***add*** we need to inform, at least ***documents***, ***metadatas*** and ***ids***.\n",
        "* In the **document** we store the big text, it's a different column in each Dataset.\n",
        "* In **metadatas**, we can informa a list of topics.\n",
        "* In **id** we need to inform an unique identificator for each row. It MUST be unique! I'm creating the ID using the range of MAX_NEWS.\n"
      ],
      "metadata": {
        "id": "hLB6eIOE3HU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collection.add(\n",
        "    documents=subset_news[DOCUMENT].tolist(),\n",
        "    metadatas=[{TOPIC: topic} for topic in subset_news[TOPIC].tolist()],\n",
        "    ids=[f\"id{x}\" for x in range(MAX_NEWS)],\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "oPxdx74K3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "results = collection.query(query_texts=[\"laptop\"], n_results=10 )\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8BAz64hR3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector MAP"
      ],
      "metadata": {
        "id": "ULKXqbLh3HU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "trusted": true,
        "id": "tIPKMhaJ3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "getado = collection.get(ids=\"id141\",\n",
        "                       include=[\"documents\", \"embeddings\"])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vaHa6HTR3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = getado[\"embeddings\"]\n",
        "word_list = getado[\"documents\"]\n",
        "word_vectors"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "O0q9hA5v3HU-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "word_list"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hvdx99Pg3HU_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have our information inside the Database we can query It, and ask for data that matches our needs. The search is done inside the content of the document, and it dosn't look for the exact word, or phrase. The results will be based on the similarity between the search terms and the content of documents.\n",
        "\n",
        "The metadata is not used in the search, but they can be utilized for filtering or refining the results after the initial search.\n"
      ],
      "metadata": {
        "id": "EcecSDap3HU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model and creating the prompt\n",
        "TRANSFORMERS!!\n",
        "Time to use the library **transformers**, the most famous library from [hugging face](https://huggingface.co/) for working with language models.\n",
        "\n",
        "We are importing:\n",
        "* **Autotokenizer**: It is a utility class for tokenizing text inputs that are compatible with various pre-trained language models.\n",
        "* **AutoModelForCasualLLM**: it provides an interface to pre-trained language models specifically designed for language generation tasks using causal language modeling (e.g., GPT models), or the model used in this notebook ***databricks/dolly-v2-3b***.\n",
        "* **pipeline**: provides a simple interface for performing various natural language processing (NLP) tasks, such as text generation (our case) or text classification.\n",
        "\n",
        "The model I have selected is [TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0), which is one of the smartest Small Language Models. Even so, it still has 1.1 billion parameters.\n",
        "\n",
        "Please, feel free to test [different Models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending), you need to search for NLP models trained for text-generation. My recomendation is choose \"small\" models, or we will run out of memory in kaggle.  \n"
      ],
      "metadata": {
        "id": "6uJF8FYF3HU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "#model_id = \"databricks/dolly-v2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "yQLBudtB3HU_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to initialize the pipeline using the objects created above.\n",
        "\n",
        "The model's response is limited to 256 tokens, for this project I'm not interested in a longer response, but it can easily be extended to whatever length you want.\n",
        "\n",
        "Setting ***device_map*** to ***auto*** we are instructing the model to automaticaly select the most appropiate device: CPU or GPU for processing the text generation.  "
      ],
      "metadata": {
        "id": "5IG719Lj3HU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=lm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZdZTNqsW3HU_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the extended prompt\n",
        "To create the prompt we use the result from query the Vector Database  and the sentence introduced by the user.\n",
        "\n",
        "The prompt have two parts, the **relevant context** that is the information recovered from the database and the **user's question**.\n",
        "\n",
        "We only need to join the two parts together to create the prompt that we are going to send to the model.\n",
        "\n",
        "You can limit the lenght of the context passed to the model, because we can get some Memory problems with one of the datasets that contains a realy large text in the document part."
      ],
      "metadata": {
        "id": "EPmgaDDz3HU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Can I buy a new Toshiba laptop?\"\n",
        "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
        "#context = context[0:5120]\n",
        "prompt_template = f\"\"\"\n",
        "Relevant context: {context}\n",
        "Considering the relevant context, answer the question.\n",
        "Question: {question}\n",
        "Answer: \"\"\"\n",
        "prompt_template"
      ],
      "metadata": {
        "trusted": true,
        "id": "RWlnmjPj3HU_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all that remains is to send the prompt to the model and wait for its response!\n"
      ],
      "metadata": {
        "id": "bFIDVXMr3HU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm_response = pipe(prompt_template)\n",
        "print(lm_response[0][\"generated_text\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "T0mByUk83HU_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions, Fork and Improve\n",
        "A very short notebook, but with a lot of content.\n",
        "\n",
        "We have used a vector database to store information. Then move on to retrieve it and use it to create an extended prompt that we've used to call one of the newer large language models available in Hugging Face.\n",
        "\n",
        "The model has returned a response to us taking into account the context that we have passed to it in the prompt.\n",
        "\n",
        "This way of working with language models is very powerful.\n",
        "\n",
        "We can make the model use our information without the need for Fine Tuning. This technique really has some very big advantages over fine tuning.\n",
        "\n",
        "Please don't stop here.\n",
        "\n",
        "* The notebook is prepared to use two more Datasets. Do tests with it.\n",
        "\n",
        "* Find another model on Hugging Face and compare it.\n",
        "\n",
        "* Modify the way to create the prompt.\n",
        "\n",
        "## Continue learning\n",
        "This notebook is part of a [course on large language models](https://github.com/peremartra/Large-Language-Model-Notebooks-Course) I'm working on and it's available on [GitHub](https://github.com/peremartra/Large-Language-Model-Notebooks-Course). You can see the other lessons and if you like it, don't forget to subscribe to receive notifications of new lessons.\n",
        "\n",
        "Other notebooks in the Large Language Models series:\n",
        "https://www.kaggle.com/code/peremartramanonellas/ask-your-documents-with-langchain-vectordb-hf"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T22:01:56.992775Z",
          "iopub.execute_input": "2023-07-12T22:01:56.993351Z",
          "iopub.status.idle": "2023-07-12T22:01:57.001309Z",
          "shell.execute_reply.started": "2023-07-12T22:01:56.993305Z",
          "shell.execute_reply": "2023-07-12T22:01:56.999431Z"
        },
        "id": "DFdkzm9o3HU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If you liked the notebook Please consider ***UPVOTING IT***. It helps others to discover it, and encourages me to continue publishing."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T22:17:34.63605Z",
          "iopub.execute_input": "2023-07-12T22:17:34.636604Z",
          "iopub.status.idle": "2023-07-12T22:17:34.644497Z",
          "shell.execute_reply.started": "2023-07-12T22:17:34.636566Z",
          "shell.execute_reply": "2023-07-12T22:17:34.642819Z"
        },
        "id": "FUVKVd603HU_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "WCAHl7CH3HVA"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}